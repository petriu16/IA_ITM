{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2018\n",
    "\n",
    "En este notebook se abordan algunas prácticas para mejorar el desempeño de una red neuronal:\n",
    "1. Normalización de la entrada\n",
    "2. Inicialización de parámetros\n",
    "3. Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Caso: El problema XOR\n",
    "\n",
    "<img src='res/shallow_nn/xor_problem.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Construyamos los bloques basicos de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W, X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivada de la funcion sigmoide, definida como $\\sigma'(z) = \\sigma(z)*(1-\\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1. - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, a):\n",
    "    return -(y * np.log(a) + (1.-y) * np.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(logloss):\n",
    "    return np.mean(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multilayer(parameters,X):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "        \n",
    "    Z1 = linear_activation(W1,b1,X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2,b2,A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    return np.round(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UTILIDADES\n",
    "'''\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(data_type, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generate a binary dataset with distribution data_type\n",
    "\n",
    "    Arguments:\n",
    "    data_type -- distribution of dataset {moons,circles,blobs}\n",
    "\n",
    "    Returns:\n",
    "    X -- features\n",
    "    Y -- labels\n",
    "    \"\"\" \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(200, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_lr(parameters, X, Y):\n",
    "    X = X.T\n",
    "    \n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict_multilayer(parameters, np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. Normalizacion de la entrada\n",
    "\n",
    "Como vimos en la sesion anterior, es deseable que la entrada del sistema se encuentre normalizada, con el objetivo de facilitar la convergencia del descenso del gradiente.\n",
    "\n",
    "Para ello utilizamos las siguientes ecuaciones:\n",
    "\n",
    "### <center>$x = x - \\mu$</center>\n",
    "### <center>$x = \\frac{x}{\\sigma^2}$</center>\n",
    "\n",
    "Donde $\\mu$ y $\\sigma^2$ son la media y varianza de la entrada $x$ respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(X):\n",
    "    x = np.copy(X)\n",
    "    \n",
    "    v_x = np.var(x)\n",
    "    m_x = np.mean(x)\n",
    "    \n",
    "    x -= m_x\n",
    "    x /= v_x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "nx,m = X.T.shape\n",
    "\n",
    "X_norm = normalize_input(X)\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "ax1.set_title(\"X original\")\n",
    "ax1.scatter(X[:,0], X[:,1], color=color)\n",
    "ax2.set_title(\"X normalizada\")\n",
    "ax2.scatter(X_norm[:,0], X_norm[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X_norm.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 2. Inicializacion de parametros de la red.\n",
    "\n",
    "Utilizar unos valores de inicializacion de los pesos de la red $W^{[l]}$, muy bajos o muy altos, puede conducir al problema de la desvanecimiento o explosion de los gradientes, respectivamente. Es por ello que se hace necesario inicializar los pesos con un valor adecuado.\n",
    "\n",
    "Segun analizamos en la clase anterior, existen aproximaciones formales para ello. Algunas formas son:\n",
    "\n",
    "### <center>$W^{[l]} = np.random.randn(W^{[l]}.shape) * \\sqrt{\\frac{1}{n^{[l-1]}}}$</center>\n",
    "\n",
    "### <center>$W^{[l]} = np.random.randn(W^{[l]}.shape) * \\sqrt{\\frac{2}{n^{[l-1]}}}$</center>\n",
    "\n",
    "Xavier's Initialization:\n",
    "### <center>$W^{[l]} = np.random.randn(W^{[l]}.shape) * \\sqrt{ \\frac{2}{n^{[l-1]} +n^{[l]} } }$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xavier's Initialization\n",
    "def initialize_parameters(neurons_hidden_layer):\n",
    "\n",
    "    len_X = 2\n",
    "    \n",
    "    W1 = np.random.randn(neurons_hidden_layer, len_X) * np.sqrt(2./float(len_X + neurons_hidden_layer))\n",
    "    b1 = np.zeros([neurons_hidden_layer,1])\n",
    "\n",
    "    W2 = np.random.randn(1,neurons_hidden_layer) * np.sqrt(2./float(neurons_hidden_layer+1))\n",
    "    b2 = np.zeros([1,1])\n",
    "    \n",
    "    parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = initialize_parameters(2)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 3. Regularization\n",
    "\n",
    "Esta tecnica permite reducir el problema de la alta varianza (High Variance), entendido como una alta diferencia entre el error medido sobre el dataset de entrenamiento (TRAIN SET) y de prueba (DEV SET). Por otra parte, permite reducir el sobreentrenamiento de la red neuronal, es decir, permite entrenas redes que encuentren soluciones mas generales.\n",
    "\n",
    "En terminos de implementacion, la regularizacion L2 o de Frobenius implica agregar un nuevo termino al gradiente de los pesos:\n",
    "\n",
    "### <center>$dW^{[l]} = dW^{[l]} + \\frac{\\lambda}{m}W^{[l]}$</center>\n",
    "\n",
    "Donde $\\lambda$ se conoce como el termino de regularizacion y $m$ se refiere al numero de ejemplos del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metaparameters initialization\n",
    "'''\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.8\n",
    "reg_param = 0.01\n",
    "\n",
    "'''\n",
    "Parameters initialization\n",
    "'''\n",
    "parameters = initialize_parameters(3)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "print (\"parametros iniciales: \", parameters)\n",
    "\n",
    "'''\n",
    "Gradient descent\n",
    "'''\n",
    "for i in range(num_epochs):\n",
    "    '''\n",
    "    Forward Propagation\n",
    "    '''\n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    '''\n",
    "    Backward Propagation\n",
    "    '''\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_sigmoid(Z1))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    '''\n",
    "    Regularization\n",
    "    '''\n",
    "    dW1 = dW1 + reg_param * W1\n",
    "    dW2 = dW2 + reg_param * W2\n",
    "    \n",
    "    '''\n",
    "    Parameters Update\n",
    "    '''\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b1 -= learning_rate * db1\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    '''\n",
    "    Cost estimation\n",
    "    '''\n",
    "    J = cost(loss(Y,A2))\n",
    "    \n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo -- iteracion \", i, \": \", J)\n",
    "        \n",
    "print(\"parametros actualizados: \", parameters)\n",
    "\n",
    "'''\n",
    "Testing\n",
    "'''\n",
    "print(\"Predicciones del clasificador: \", predict_multilayer(parameters,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Visualizacion del resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_lr(parameters, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "# Caso 2: Dataset generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z)- np.exp(-z))/(np.exp(z)+ np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_tanh(z):\n",
    "    return 1. - tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multilayer(parameters,X):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "        \n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    return np.round(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('moons')\n",
    "nx,m = X.T.shape\n",
    "\n",
    "X = normalize_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = normalize_input(X)\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "ax1.set_title(\"X original\")\n",
    "ax1.scatter(X[:,0], X[:,1], color=color)\n",
    "ax2.set_title(\"X normalizada\")\n",
    "ax2.scatter(X_norm[:,0], X_norm[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X_norm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metaparameters initialization\n",
    "'''\n",
    "num_epochs = 50000\n",
    "learning_rate = 0.9\n",
    "reg_param = 0.0\n",
    "\n",
    "'''\n",
    "Parameters initialization\n",
    "'''\n",
    "parameters = initialize_parameters(10)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "print (\"parametros iniciales: \", parameters)\n",
    "\n",
    "'''\n",
    "Gradient descent\n",
    "'''\n",
    "for i in range(num_epochs):\n",
    "    '''\n",
    "    Forward Propagation\n",
    "    '''\n",
    "    Z1 = linear_activation(W1, b1, X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2, b2, A1)\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    '''\n",
    "    Backward Propagation\n",
    "    '''\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), d_sigmoid(Z1))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    '''\n",
    "    Regularization\n",
    "    '''\n",
    "    dW1 = dW1 + reg_param * W1\n",
    "    dW2 = dW2 + reg_param * W2\n",
    "        \n",
    "    '''\n",
    "    Parameters Update\n",
    "    '''\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    '''\n",
    "    Cost estimation\n",
    "    '''\n",
    "    J = cost(loss(Y,A2))\n",
    "    \n",
    "    \n",
    "    if(i%1000 == 0):\n",
    "        print(\"costo -- iteracion \", i, \": \", J)\n",
    "        \n",
    "print(\"parametros actualizados: \", parameters)\n",
    "\n",
    "'''\n",
    "Testing\n",
    "'''\n",
    "print(\"Predicciones del clasificador: \", predict_multilayer(parameters,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_lr(parameters, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
