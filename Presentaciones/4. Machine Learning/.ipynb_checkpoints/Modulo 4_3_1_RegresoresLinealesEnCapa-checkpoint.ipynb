{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n",
    "\n",
    "## Inteligencia Artificial - IAI84\n",
    "### Instituto Tecnológico Metropolitano\n",
    "#### Pedro Atencio Ortiz - 2018\n",
    "\n",
    "En este notebook se aborda el tema de aprendizaje de máquina para clasificación binaria no-lineal utilizando Regresores logísticos en cadena:\n",
    "1. El problema XOR\n",
    "2. Regresores logísticos en cadena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# 1. El problema XOR\n",
    "\n",
    "<img src='res/shallow_nn/xor_problem.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Regresor Logístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W.T,X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Returns sigmoid activation for array z\n",
    "    '''\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y, a):\n",
    "    return -(y * np.log(a) + (1-y) * np.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logloss):\n",
    "    return np.mean(logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Trabajemos\n",
    "3. Realicemos descenso del gradiente por cada regresor logístico.\n",
    "4. Creemos un solo regresor no lineal combinando el grafo de cómputo en cadena visto en clase y los W y b actualizados para cada regresor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creemos los 3 datasets (X1, Y1), (X2, Y2) y (X3, Y3) según trabajamos en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X1: ', array([[0, 0, 1, 1],\n",
      "       [0, 1, 0, 1]]), 'Y1: ', array([[0, 1, 1, 1]]))\n",
      "('X2: ', array([[0, 0, 1, 1],\n",
      "       [0, 1, 0, 1]]), 'Y2: ', array([[1, 1, 1, 0]]))\n",
      "('X3: ', array([[0, 1, 1, 1],\n",
      "       [1, 1, 1, 0]]), 'Y3: ', array([[0, 1, 1, 0]]))\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([[0,0],[0,1],[1,0],[1,1]]).T\n",
    "Y1 = np.array([[0, 1, 1, 1]])\n",
    "print(\"X1: \",X1, 'Y1: ', Y1)\n",
    "\n",
    "X2 = None\n",
    "Y2 = None\n",
    "print(\"X2: \",X2, 'Y2: ', Y2)\n",
    "\n",
    "X3 = None\n",
    "Y3 = None\n",
    "print(\"X3: \",X3, 'Y3: ', Y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado:__<br>\n",
    "('X1: ', array([[0, 0, 1, 1],\n",
    "       [0, 1, 0, 1]]), 'Y1: ', array([[0, 1, 1, 1]]))\n",
    "       <br>\n",
    "('X2: ', array([[0, 0, 1, 1],\n",
    "       [0, 1, 0, 1]]), 'Y2: ', array([[1, 1, 1, 0]]))\n",
    "       <br>\n",
    "('X3: ', array([[0, 1, 1, 1],\n",
    "       [1, 1, 1, 0]]), 'Y3: ', array([[0, 1, 1, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Inicialicemos los pesos W1, W2, W3 de forma aleatoria (__np.random.rand(shape)__) y los bias b en zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('W1: ', array([[-0.41675785],\n",
      "       [-0.05626683]]), 'b1: ', 0)\n",
      "('W2: ', array([[-2.1361961 ],\n",
      "       [ 1.64027081]]), 'b2: ', 0)\n",
      "('W3: ', array([[-1.79343559],\n",
      "       [-0.84174737]]), 'b3: ', 0)\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters():\n",
    "    seed = 2\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    W1 = None\n",
    "    b1 = 0\n",
    "\n",
    "    W2 = None\n",
    "    b2 = 0\n",
    "\n",
    "    W3 = None\n",
    "    b3 = 0\n",
    "\n",
    "    return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "(W1,b1,W2,b2,W3,b3) = initialize_parameters()\n",
    "\n",
    "print('W1: ', W1, 'b1: ',b1)\n",
    "print('W2: ', W2, 'b2: ',b2)\n",
    "print('W3: ', W3, 'b3: ',b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado:__\n",
    "<br>\n",
    "('W1: ', array([[-0.41675785],\n",
    "       [-0.05626683]]), 'b1: ', 0)\n",
    "       <br>\n",
    "('W2: ', array([[-2.1361961 ],\n",
    "       [ 1.64027081]]), 'b2: ', 0)\n",
    "       <br>\n",
    "('W3: ', array([[-1.79343559],\n",
    "       [-0.84174737]]), 'b3: ', 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### - Apliquemos descenso del gradiente a cada regresor logístico por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(A, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dz = A - Y\n",
    "    dW = np.dot(X, dz.T) / m\n",
    "    db = np.sum(dz) / m\n",
    "    \n",
    "    return (dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('costo regresor 1 -- iteracion ', 0, ': ', 0.82381919092736267)\n",
      "('costo regresor 2 -- iteracion ', 0, ': ', 1.690717816357759)\n",
      "('costo regresor 3 -- iteracion ', 0, ': ', 1.690717816357759)\n",
      "('costo regresor 1 -- iteracion ', 200, ': ', 0.27825607548403219)\n",
      "('costo regresor 2 -- iteracion ', 200, ': ', 0.44490234018622971)\n",
      "('costo regresor 3 -- iteracion ', 200, ': ', 0.44490234018622971)\n",
      "('costo regresor 1 -- iteracion ', 400, ': ', 0.19856232651835581)\n",
      "('costo regresor 2 -- iteracion ', 400, ': ', 0.40908327256698895)\n",
      "('costo regresor 3 -- iteracion ', 400, ': ', 0.40908327256698895)\n",
      "('costo regresor 1 -- iteracion ', 600, ': ', 0.15194782201885557)\n",
      "('costo regresor 2 -- iteracion ', 600, ': ', 0.39284340203102042)\n",
      "('costo regresor 3 -- iteracion ', 600, ': ', 0.39284340203102042)\n",
      "('costo regresor 1 -- iteracion ', 800, ': ', 0.12192724366813479)\n",
      "('costo regresor 2 -- iteracion ', 800, ': ', 0.38874474786568558)\n",
      "('costo regresor 3 -- iteracion ', 800, ': ', 0.38874474786568558)\n",
      "('costo regresor 1 -- iteracion ', 1000, ': ', 0.10123752647565586)\n",
      "('costo regresor 2 -- iteracion ', 1000, ': ', 0.39205226767073698)\n",
      "('costo regresor 3 -- iteracion ', 1000, ': ', 0.39205226767073698)\n",
      "('costo regresor 1 -- iteracion ', 1200, ': ', 0.086231814097234294)\n",
      "('costo regresor 2 -- iteracion ', 1200, ': ', 0.39980268820065185)\n",
      "('costo regresor 3 -- iteracion ', 1200, ': ', 0.39980268820065185)\n",
      "('costo regresor 1 -- iteracion ', 1400, ': ', 0.074911294019162036)\n",
      "('costo regresor 2 -- iteracion ', 1400, ': ', 0.4101614162212282)\n",
      "('costo regresor 3 -- iteracion ', 1400, ': ', 0.4101614162212282)\n",
      "('costo regresor 1 -- iteracion ', 1600, ': ', 0.066100298053191869)\n",
      "('costo regresor 2 -- iteracion ', 1600, ': ', 0.42199148974719164)\n",
      "('costo regresor 3 -- iteracion ', 1600, ': ', 0.42199148974719164)\n",
      "('costo regresor 1 -- iteracion ', 1800, ': ', 0.059067083863525841)\n",
      "('costo regresor 2 -- iteracion ', 1800, ': ', 0.43458474800401775)\n",
      "('costo regresor 3 -- iteracion ', 1800, ': ', 0.43458474800401775)\n",
      "('W1 actualizado: ', array([[ 4.97542106],\n",
      "       [ 5.33591208]]), 'b1 actualizado: ', -1.9635918185739658)\n",
      "('W2 actualizado: ', array([[-5.78608353],\n",
      "       [-2.00961662]]), 'b2 actualizado: ', 6.4907942486938373)\n",
      "('W3 actualizado: ', array([[ 3.32968151],\n",
      "       [ 4.28136973]]), 'b3 actualizado: ', -5.4722854982861442)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "(W1,b1,W2,b2,W3,b3) = initialize_parameters()\n",
    "\n",
    "for i in range(2000): #2000 iteraciones del descenso del gradiente\n",
    "    ## Forward Propagation -- a = sigmoid(z = linear_activation)\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    \n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    \n",
    "    Z3 = None\n",
    "    A3 = None\n",
    "    \n",
    "    #Backward Propagation -- backward_propagation(A, X, Y)\n",
    "    (dW1, db1) = None\n",
    "    (dW2, db2) = None\n",
    "    (dW3, db3) = None\n",
    "    \n",
    "    #Weight and bias Updates W = W - learning * dW  ---- b = b - learning_rate * db\n",
    "    W1 -= None\n",
    "    b1 -= None\n",
    "    W2 -= None\n",
    "    b2 -= None\n",
    "    W3 -= None\n",
    "    b3 -= None\n",
    "    \n",
    "    #Cost estimation\n",
    "    J1 = cost(loss(Y1,A1))\n",
    "    J2 = cost(loss(Y2,A3))\n",
    "    J3 = cost(loss(Y3,A3))\n",
    "    \n",
    "    \n",
    "    if(i%200 == 0):\n",
    "        print(\"costo regresor 1 -- iteracion \", i, \": \", J1)\n",
    "        print(\"costo regresor 2 -- iteracion \", i, \": \", J2)\n",
    "        print(\"costo regresor 3 -- iteracion \", i, \": \", J2)\n",
    "\n",
    "print(\"W1 actualizado: \",W1, \"b1 actualizado: \", b1)\n",
    "print(\"W2 actualizado: \",W2, \"b2 actualizado: \", b2)\n",
    "print(\"W3 actualizado: \",W3, \"b3 actualizado: \", b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado:__\n",
    "<br>\n",
    "('costo regresor 1 -- iteracion ', 0, ': ', 0.82381919092736267)\n",
    "<br>\n",
    "('costo regresor 2 -- iteracion ', 0, ': ', 1.690717816357759)\n",
    "<br>\n",
    "('costo regresor 3 -- iteracion ', 0, ': ', 1.690717816357759)\n",
    "<br>\n",
    "...\n",
    "<br>\n",
    "('costo regresor 1 -- iteracion ', 1800, ': ', 0.059067083863525841)\n",
    "<br>\n",
    "('costo regresor 2 -- iteracion ', 1800, ': ', 0.43458474800401775)\n",
    "<br>\n",
    "('costo regresor 3 -- iteracion ', 1800, ': ', 0.43458474800401775)\n",
    "<br><br>\n",
    "('W1 actualizado: ', array([[ 3.63883826],\n",
    "       [ 3.99932928]]), 'b1 actualizado: ', -1.2304228512033604)\n",
    "       <br>\n",
    "('W2 actualizado: ', array([[-4.46040679],\n",
    "       [-0.68393989]]), 'b2 actualizado: ', 4.440351150899609)\n",
    "       <br>\n",
    "('W3 actualizado: ', array([[ 1.97706174],\n",
    "       [ 2.92874996]]), 'b3 actualizado: ', -3.3939910248821952)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - verifiquemos las predicciones por cada regresor logístico ya entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(W,b,X):\n",
    "    z = linear_activation(W,b,X)\n",
    "    A = sigmoid(z)\n",
    "    return np.round(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predicciones regresor 1: ', array([[ 0.,  1.,  1.,  1.]]), '--- Clases originales: ', array([[0, 1, 1, 1]]))\n",
      "('predicciones regresor 2: ', array([[ 1.,  1.,  1.,  0.]]), '--- Clases originales: ', array([[1, 1, 1, 0]]))\n",
      "('predicciones regresor 3: ', array([[ 0.,  1.,  1.,  0.]]), '--- Clases originales: ', array([[0, 1, 1, 0]]))\n"
     ]
    }
   ],
   "source": [
    "Y_hat1 = predict(W1,b1,X1)\n",
    "Y_hat2 = predict(W2,b2,X2)\n",
    "Y_hat3 = predict(W3,b3,X3)\n",
    "print(\"predicciones regresor 1: \",np.round(Y_hat1),\"--- Clases originales: \", Y1)\n",
    "print(\"predicciones regresor 2: \",np.round(Y_hat2),\"--- Clases originales: \", Y2)\n",
    "print(\"predicciones regresor 3: \",np.round(Y_hat3),\"--- Clases originales: \", Y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado:__\n",
    "<br>\n",
    "('predicciones regresor 1: ', array([[ 0.,  1.,  1.,  1.]]), '--- Clases originales: ', array([[0, 1, 1, 1]]))\n",
    "<br>\n",
    "('predicciones regresor 2: ', array([[ 1.,  1.,  1.,  0.]]), '--- Clases originales: ', array([[1, 1, 1, 0]]))\n",
    "<br>\n",
    "('predicciones regresor 3: ', array([[ 0.,  1.,  1.,  0.]]), '--- Clases originales: ', array([[0, 1, 1, 0]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### - Agrupemos los tres regresores en capas\n",
    "<img src='res/shallow_nn/compute_graph_3.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def predict_multilayer(W1,b1,W2,b2,W3,b3,X):\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    \n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    \n",
    "    #A1 and A2 are inputs for z3. Use np.concatenate((A1,A2),axis=0) to create X3\n",
    "    X3 = None\n",
    "    Z3 = None\n",
    "    A3 = None\n",
    "    \n",
    "    return np.round(A3)\n",
    "\n",
    "y_hat = predict_multilayer(W1,b1,W2,b2,W3,b3,X1)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resultado esperado:__[[ 0.  1.  1.  0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apliquemos nuestro regresor multicapa al problema XOR\n",
    "----- Solo ejecutar celdas -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAEyCAYAAABNk1+cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSrn/wFLFuqMsTojmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11a80cbb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_lr(W1, b1, W2, b2, W3, b3, X, Y):\n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    #Z = pred_func(W,b,np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = predict_multilayer(W1,b1,W2,b2,W3,b3,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    print(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_lr(W1, b1, W2, b2, W3, b3, X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
